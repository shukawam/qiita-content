---
title: Hugging Face TGI(Text Generation Inference) ã‚’ OCI ã® A10 ã§è©¦ã—ã¦ã¿ãŸ
tags:
  - "oci"
  - "huggingface"
private: false
updated_at: ""
id: null
organization_url_name: oracle
slide: false
ignorePublish: false
---

# ã¯ã˜ã‚ã«

LLM(Large Language Model/å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«)ã®æ¨è«–ç’°å¢ƒã‚’ API ã¨ã—ã¦æä¾›ã™ã‚‹å ´åˆã€Fast API, Flask ãªã©ã§æ¨è«–éƒ¨åˆ†ã‚’ãƒ©ãƒƒãƒ—ã—ã¦æä¾›ã™ã‚‹æ–¹æ³•ã®ä»–ã« Hugging Face ã® TGI(Text Generation Inference)ã‚’ä½¿ã†æ–¹æ³•ãŒã‚ã‚Šã¾ã™ã€‚è‡ªå‰ã§å®Ÿè£…ã™ã‚‹å ´åˆã€ç´°ã‹ãèª¿æ•´ã‚’ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ãŒã€ãã®åˆ†è¤‡æ•°ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆ`max_new_tokens`, `top_k`, ...ï¼‰ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹å¿…è¦ãŒã‚ã£ãŸã‚Šã¨ã€å®Ÿè£…ã¯ãã‚Œãªã‚Šã«æ‰‹é–“ãŒã‹ã‹ã‚‹ã‚‚ã®ã§ã™ã€‚TGI ã§ã¯ã€ä¸»è¦ãªã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã®ã§ã€ãƒ­ãƒ¼ã‚«ãƒ«ã§æ‰‹è»½ã«è©¦ã™åˆ†ã«ã¯éå¸¸ã«ä½“é¨“ãŒè‰¯ã„ã‚‚ã®ã ã¨æ€ã„ã¾ã—ãŸã€‚ãã“ã§ã€ä»Šå›ã¯ã‚­ãƒ£ãƒƒãƒã‚¢ãƒƒãƒ—ãŒã¦ã‚‰ Hugging Face TGI ã‚’ OCI ã‹ã‚‰æä¾›ã•ã‚Œã¦ã„ã‚‹ A10 ã‚’ä½¿ã£ã¦è©¦ã—ã¦ã¿ãŸã„ã¨æ€ã„ã¾ã™ã€‚

# Hugging Face TGI

Hugging Face TGI ã¯ã€Rust, Python ã§å®Ÿè£…ã•ã‚Œã¦ã„ã‚‹ LLM ã®æ¨è«– API ã‚’æä¾›ã™ã‚‹ãŸã‚ã®ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚

![TGI.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/560068/dc2a1291-a5cf-4a06-7326-03e9a051374d.png)

å¼•ç”¨: [Hugging Face - Text Generation Inference](https://huggingface.co/docs/text-generation-inference/index)

å›³ã‚’è¦‹ã¦ã‚‚ã‚‰ã†ã¨åˆ†ã‹ã‚‹é€šã‚Šã€Web Server éƒ¨åˆ†ã¯ Rust ã§å®Ÿè£…ã•ã‚Œã¦ã„ã¦ã€ãã®è£å´ã®æ¨è«–éƒ¨åˆ†ã¯ Python ã§å®Ÿè£…ã•ã‚Œã¦ãŠã‚Šã€ãã‚ŒãŒ gRPC çµŒç”±ã§å‘¼ã³å‡ºã•ã‚Œã¦ã„ã¾ã™ã€‚ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¯ã€[https://huggingface.github.io/text-generation-inference/](https://huggingface.github.io/text-generation-inference/) ã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚ä¸€è¦‹ã—ã¦ã¿ã‚‹ã¨ã€ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã®ãŸã‚ã® `/generate` ã‚„ãã‚Œã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡ºåŠ›ã™ã‚‹ãŸã‚ã® `/generate_stream`ã€å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã™ã‚‹ãŸã‚ã® `/tokenize`ã€ãƒãƒ£ãƒƒãƒˆç”¨ã® `/v1/chat/completions`, `/v1/completions` ãªã©ãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ã€‚

ã¾ãŸã€OpenTelemetry ã«ã‚ˆã‚‹åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ãŸã‚Šã€OpenMetrics å½¢å¼ã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å…¬é–‹ã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆ`/metrics`ï¼‰ã‚’æœ‰ã—ã¦ã„ãŸã‚Šã¨ã‚ªãƒ–ã‚¶ãƒ¼ãƒãƒ“ãƒªãƒ†ã‚£ã«é–¢ã™ã‚‹ä»•çµ„ã¿ã‚‚ååˆ†ã«å‚™ã‚ã£ã¦ã„ã‚‹ã¿ãŸã„ã§ã™ã€‚

# OCI ã® A10 ä¸Šã§ TGI ã‚’è©¦ã—ã¦ã¿ã‚‹

## å‰æ

- OCI ä¸Šã« A10(VM)ãŒæ§‹ç¯‰ã•ã‚Œã¦ã„ã‚‹ã“ã¨
- LLM ã¯ä»»æ„ã§ã™ãŒã€ä»Šå›ã¯ [Swallow-7b-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-hf) ã‚’ä½¿ç”¨ã—ã¾ã™
- Docker ã«é–¢ã™ã‚‹åŸºæœ¬çš„ãªæ“ä½œãŒã§ãã‚‹ã“ã¨
- NVIDIA Container Toolkit ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ã§ã‚ã‚‹ã“ã¨
  - æœªå®Ÿæ–½ã®æ–¹ã¯ã€[https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) ã‚’ã”å‚ç…§ãã ã•ã„

## Swallow-7b-hf ã®æ¨è«–ç’°å¢ƒã‚’æ§‹ç¯‰ã—ã¦ã¿ã‚‹

ä»¥ä¸‹ã®ã‚ˆã†ã«èµ·å‹•ã—ã¾ã™ã€‚

```sh
docker container run --gpus all --shm-size 64g -p 8080:80 \
  -v $PWD/data:/data \
  ghcr.io/huggingface/text-generation-inference:2.0.3 \
  --model-id tokyotech-llm/Swallow-7b-hf
```

ãƒœãƒªãƒ¥ãƒ¼ãƒ ãƒã‚¦ãƒ³ãƒˆã‚’ã—ã¦ãŠãã¨ã€å®Ÿè¡Œã®ãŸã³ã«ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†ãŒèµ°ã‚‹ã“ã¨ãŒãªããªã‚‹ã®ã§ã€å…¥ã‚Œã¦ãŠãã®ãŒè‰¯ã„ã§ã™ã€‚

å®Ÿè¡Œã™ã‚‹ã¨ã€ä»¥ä¸‹ã®ã‚ˆã†ãªãƒ­ã‚°ãŒå‡ºåŠ›ã•ã‚Œã¾ã™ã€‚ï¼ˆåˆå›å®Ÿè¡Œæ™‚ã«ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†ãŒèµ°ã‚‹ãŸã‚å°‘ã—æ™‚é–“ã‚’è¦ã—ã¾ã™ï¼‰

```log
2024-05-31T05:53:06.994911Z  INFO text_generation_launcher: Args {
    model_id: "tokyotech-llm/Swallow-7b-hf",
    revision: None,
    validation_workers: 2,
    sharded: None,
    num_shard: None,
    quantize: None,
    speculate: None,
    dtype: None,
    trust_remote_code: false,
    max_concurrent_requests: 128,
    max_best_of: 2,
    max_stop_sequences: 4,
    max_top_n_tokens: 5,
    max_input_tokens: None,
    max_input_length: None,
    max_total_tokens: None,
    waiting_served_ratio: 0.3,
    max_batch_prefill_tokens: None,
    max_batch_total_tokens: None,
    max_waiting_tokens: 20,
    max_batch_size: None,
    cuda_graphs: None,
    hostname: "4ffe1b16383f",
    port: 80,
    shard_uds_path: "/tmp/text-generation-server",
    master_addr: "localhost",
    master_port: 29500,
    huggingface_hub_cache: Some(
        "/data",
    ),
    weights_cache_override: None,
    disable_custom_kernels: false,
    cuda_memory_fraction: 1.0,
    rope_scaling: None,
    rope_factor: None,
    json_output: false,
    otlp_endpoint: None,
    cors_allow_origin: [],
    watermark_gamma: None,
    watermark_delta: None,
    ngrok: false,
    ngrok_authtoken: None,
    ngrok_edge: None,
    tokenizer_config_path: None,
    disable_grammar_support: false,
    env: false,
    max_client_batch_size: 4,
}
2024-05-31T05:53:06.994984Z  INFO hf_hub: Token file not found "/root/.cache/huggingface/token"
2024-05-31T05:53:07.377899Z  INFO text_generation_launcher: Default `max_input_tokens` to 4095
2024-05-31T05:53:07.377913Z  INFO text_generation_launcher: Default `max_total_tokens` to 4096
2024-05-31T05:53:07.377915Z  INFO text_generation_launcher: Default `max_batch_prefill_tokens` to 4145
2024-05-31T05:53:07.377917Z  INFO text_generation_launcher: Using default cuda graphs [1, 2, 4, 8, 16, 32]
2024-05-31T05:53:07.377987Z  INFO download: text_generation_launcher: Starting download process.
2024-05-31T05:53:11.016162Z  WARN text_generation_launcher: No safetensors weights found for model tokyotech-llm/Swallow-7b-hf at revision None. Downloading PyTorch weights.

2024-05-31T05:53:11.196666Z  INFO text_generation_launcher: Download file: pytorch_model-00001-of-00002.bin

2024-05-31T05:53:25.226283Z  INFO text_generation_launcher: Downloaded /data/models--tokyotech-llm--Swallow-7b-hf/snapshots/84f3f991fe44779f2995796ef6478c81d4456c9d/pytorch_model-00001-of-00002.bin in 0:00:14.

2024-05-31T05:53:25.226358Z  INFO text_generation_launcher: Download: [1/2] -- ETA: 0:00:14

2024-05-31T05:53:25.226592Z  INFO text_generation_launcher: Download file: pytorch_model-00002-of-00002.bin

2024-05-31T05:53:30.904069Z  INFO text_generation_launcher: Downloaded /data/models--tokyotech-llm--Swallow-7b-hf/snapshots/84f3f991fe44779f2995796ef6478c81d4456c9d/pytorch_model-00002-of-00002.bin in 0:00:05.

2024-05-31T05:53:30.904152Z  INFO text_generation_launcher: Download: [2/2] -- ETA: 0

2024-05-31T05:53:30.904227Z  WARN text_generation_launcher: ğŸš¨ğŸš¨BREAKING CHANGE in 2.0ğŸš¨ğŸš¨: Safetensors conversion is disabled without `--trust-remote-code` because Pickle files are unsafe and can essentially contain remote code execution!Please check for more information here: https://huggingface.co/docs/text-generation-inference/basic_tutorials/safety

2024-05-31T05:53:30.904287Z  WARN text_generation_launcher: No safetensors weights found for model tokyotech-llm/Swallow-7b-hf at revision None. Converting PyTorch weights to safetensors.

2024-05-31T05:53:46.719463Z  INFO text_generation_launcher: Convert: [1/2] -- Took: 0:00:15.451689

2024-05-31T05:53:52.433858Z  INFO text_generation_launcher: Convert: [2/2] -- Took: 0:00:05.714075

2024-05-31T05:53:52.913530Z  INFO download: text_generation_launcher: Successfully downloaded weights.
2024-05-31T05:53:53.041688Z  INFO shard-manager: text_generation_launcher: Starting shard rank=0
2024-05-31T05:54:02.056947Z  INFO text_generation_launcher: Server started at unix:///tmp/text-generation-server-0

2024-05-31T05:54:02.150228Z  INFO shard-manager: text_generation_launcher: Shard ready in 9.107832275s rank=0
2024-05-31T05:54:02.220946Z  INFO text_generation_launcher: Starting Webserver
2024-05-31T05:54:02.242553Z  INFO text_generation_router: router/src/main.rs:195: Using the Hugging Face API
2024-05-31T05:54:02.242597Z  INFO hf_hub: /usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/hf-hub-0.3.2/src/lib.rs:55: Token file not found "/root/.cache/huggingface/token"
2024-05-31T05:54:02.822118Z  INFO text_generation_router: router/src/main.rs:474: Serving revision 84f3f991fe44779f2995796ef6478c81d4456c9d of model tokyotech-llm/Swallow-7b-hf
2024-05-31T05:54:02.822185Z  INFO text_generation_router: router/src/main.rs:289: Using config Some(Llama)
2024-05-31T05:54:02.822197Z  WARN text_generation_router: router/src/main.rs:291: Could not find a fast tokenizer implementation for tokyotech-llm/Swallow-7b-hf
2024-05-31T05:54:02.822199Z  WARN text_generation_router: router/src/main.rs:292: Rust input length validation and truncation is disabled
2024-05-31T05:54:02.825450Z  INFO text_generation_router: router/src/main.rs:317: Warming up model
2024-05-31T05:54:04.410417Z  INFO text_generation_launcher: Cuda Graphs are enabled for sizes [1, 2, 4, 8, 16, 32]

2024-05-31T05:54:05.264187Z  INFO text_generation_router: router/src/main.rs:354: Setting max batch total tokens to 15232
2024-05-31T05:54:05.264204Z  INFO text_generation_router: router/src/main.rs:355: Connected
2024-05-31T05:54:05.264207Z  WARN text_generation_router: router/src/main.rs:369: Invalid hostname, defaulting to 0.0.0.0
```

## LLM ã®æ¨è«–ã‚’å®Ÿæ–½ã—ã¦ã¿ã‚‹

### with curl

ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆåŒæœŸï¼‰

```sh
curl http://localhost:8080/generate \
    -X POST \
    -d '{"inputs":"æ—¥æœ¬ã§ä¸€ç•ªé«˜ã„å±±ã¯ãªã‚“ã§ã™ã‹ï¼Ÿ","parameters":{"max_new_tokens":200}}' \
    -H 'Content-Type: application/json'
```

å®Ÿè¡Œçµæœ:

```sh
{"generated_text":"\næ—¥æœ¬ã§ä¸€ç•ªé«˜ã„å±±ã¯ãªã‚“ã§ã™ã‹?\nãƒ™ã‚¹ãƒˆã‚¢ãƒ³ã‚µãƒ¼\nå¯Œå£«å±±ã§ã™ã€‚ æ—¥æœ¬ã§ä¸€ç•ªé«˜ã„å±±ã¯å¯Œå£«å±±ã§ã™ã€‚ å¯Œå£«å±±ã¯ã€é™å²¡çœŒã¨å±±æ¢¨çœŒã«ã¾ãŸãŒã‚‹æ¨™é«˜3,776mã®å±±ã§ã™ã€‚ æ—¥æœ¬ã§ä¸€ç•ªé«˜ã„å±±ã¯å¯Œå£«å±±ã§ã™ã€‚ å¯Œå£«å±±ã¯ã€é™å²¡çœŒã¨å±±æ¢¨çœŒã«ã¾ãŸãŒã‚‹æ¨™é«˜3,776mã®å±±ã§ã™ã€‚ å¯Œå£«å±±ã¯ã€é™å²¡çœŒã¨å±±æ¢¨çœŒã«ã¾ãŸãŒã‚‹æ¨™é«˜3,776mã®å±±ã§ã™ã€‚ å¯Œå£«å±±ã¯ã€é™å²¡çœŒã¨å±±æ¢¨çœŒã«ã¾ãŸãŒã‚‹æ¨™é«˜3,776mã®å±±ã§ã™ã€‚ å¯Œå£«å±±ã¯ã€é™å²¡çœŒã¨å±±æ¢¨çœŒã«ã¾ãŸãŒã‚‹æ¨™é«˜3,776mã®å±±ã§ã™ã€‚ å¯Œå£«å±±ã¯ã€é™å²¡çœŒã¨å±±æ¢¨çœŒã«ã¾ãŸãŒã‚‹æ¨™é«˜3,776mã®å±±ã§ã™ã€‚ å¯Œå£«å±±ã¯ã€é™å²¡çœŒã¨å±±æ¢¨çœŒã«ã¾ãŸãŒã‚‹æ¨™é«˜"}
```

ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒ å‡ºåŠ›ï¼‰

```sh
curl http://localhost:8080/generate_stream \
    -X POST \
    -d '{"inputs":"æ—¥æœ¬ã§ä¸€ç•ªé«˜ã„å±±ã¯ãªã‚“ã§ã™ã‹ï¼Ÿ","parameters":{"max_new_tokens":20}}' \
    -H 'Content-Type: application/json'
```

å®Ÿè¡Œçµæœ:

```sh
data:{"index":1,"token":{"id":13,"text":"\n","logprob":-0.5283203,"special":false},"generated_text":null,"details":null}

data:{"index":2,"token":{"id":32072,"text":"æ—¥æœ¬","logprob":-2.4394531,"special":false},"generated_text":null,"details":null}

data:{"index":3,"token":{"id":30499,"text":"ã§","logprob":-0.54785156,"special":false},"generated_text":null,"details":null}

data:{"index":4,"token":{"id":32611,"text":"ä¸€ç•ª","logprob":-0.114868164,"special":false},"generated_text":null,"details":null}

data:{"index":5,"token":{"id":32246,"text":"é«˜ã„","logprob":-0.02911377,"special":false},"generated_text":null,"details":null}

data:{"index":6,"token":{"id":30329,"text":"å±±","logprob":-0.015052795,"special":false},"generated_text":null,"details":null}

data:{"index":7,"token":{"id":30449,"text":"ã¯","logprob":-0.18432617,"special":false},"generated_text":null,"details":null}

data:{"index":8,"token":{"id":32126,"text":"ãªã‚“","logprob":-1.1835938,"special":false},"generated_text":null,"details":null}

data:{"index":9,"token":{"id":32001,"text":"ã§ã™","logprob":-0.012969971,"special":false},"generated_text":null,"details":null}

data:{"index":10,"token":{"id":30412,"text":"ã‹","logprob":-0.0022029877,"special":false},"generated_text":null,"details":null}

data:{"index":11,"token":{"id":29973,"text":"?","logprob":-0.06573486,"special":false},"generated_text":null,"details":null}

data:{"index":12,"token":{"id":13,"text":"\n","logprob":-0.19824219,"special":false},"generated_text":null,"details":null}

data:{"index":13,"token":{"id":33676,"text":"ãƒ™ã‚¹ãƒˆ","logprob":-1.2333984,"special":false},"generated_text":null,"details":null}

data:{"index":14,"token":{"id":32709,"text":"ã‚¢ãƒ³","logprob":-0.00019824505,"special":false},"generated_text":null,"details":null}

data:{"index":15,"token":{"id":32119,"text":"ã‚µãƒ¼","logprob":-0.0000014305115,"special":false},"generated_text":null,"details":null}

data:{"index":16,"token":{"id":13,"text":"\n","logprob":-0.0010671616,"special":false},"generated_text":null,"details":null}

data:{"index":17,"token":{"id":35072,"text":"å¯Œå£«","logprob":-0.8286133,"special":false},"generated_text":null,"details":null}

data:{"index":18,"token":{"id":30329,"text":"å±±","logprob":-0.003786087,"special":false},"generated_text":null,"details":null}

data:{"index":19,"token":{"id":32001,"text":"ã§ã™","logprob":-0.58496094,"special":false},"generated_text":null,"details":null}

data:{"index":20,"token":{"id":30267,"text":"ã€‚","logprob":-0.34350586,"special":false},"generated_text":"\næ—¥æœ¬ã§ä¸€ç•ªé«˜ã„å±±ã¯ãªã‚“ã§ã™ã‹?\nãƒ™ã‚¹ãƒˆã‚¢ãƒ³ã‚µãƒ¼\nå¯Œå£«å±±ã§ã™ã€‚","details":null}
```

### with Inference Client

Python ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ä¸Šã‹ã‚‰å®Ÿè¡Œã™ã‚‹å ´åˆã¯ã€[huggingface-hub](https://huggingface.co/docs/huggingface_hub/main/en/index) ã‹ã‚‰ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒæä¾›ã•ã‚Œã¦ã„ã‚‹ã®ã§ãã‚Œã‚’ä½¿ã†ã®ãŒè‰¯ã„ã§ã—ã‚‡ã†ã€‚

```sh
pip install huggingface-hub
```

ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ

```py
from huggingface_hub import InferenceClient

client = InferenceClient(model="http://localhost:8080")
response = client.text_generation(prompt="æ—¥æœ¬ã§ä¸€ç•ªé«˜ã„å±±ã¯ãªã‚“ã§ã™ã‹ï¼Ÿ", max_new_tokens=200)
print(response)
```

å®Ÿè¡Œçµæœ:

```py
æ—¥æœ¬ã§ä¸€ç•ªé«˜ã„å±±ã¯ãªã‚“ã§ã™ã‹?
ãƒ™ã‚¹ãƒˆã‚¢ãƒ³ã‚µãƒ¼
å¯Œå£«å±±ã§ã™ã€‚ æ—¥æœ¬ã§ä¸€ç•ªé«˜ã„å±±ã¯å¯Œå£«å±±ã§ã™ã€‚ å¯Œå£«å±±ã¯ã€é™å²¡çœŒã¨å±±æ¢¨çœŒã«ã¾ãŸãŒã‚‹æ¨™é«˜3,776mã®å±±ã§ã™ã€‚ æ—¥æœ¬ã§ä¸€ç•ªé«˜ã„å±±ã¯å¯Œå£«å±±ã§ã™ã€‚ å¯Œå£«å±±ã¯ã€é™å²¡çœŒã¨å±±æ¢¨çœŒã«ã¾ãŸãŒã‚‹æ¨™é«˜3,776mã®å±±ã§ã™ã€‚ å¯Œå£«å±±ã¯ã€é™å²¡çœŒã¨å±±æ¢¨çœŒã«ã¾ãŸãŒã‚‹æ¨™é«˜3,776mã®å±±ã§ã™ã€‚ å¯Œå£«å±±ã¯ã€é™å²¡çœŒã¨å±±æ¢¨çœŒã«ã¾ãŸãŒã‚‹æ¨™é«˜3,776mã®å±±ã§ã™ã€‚ å¯Œå£«å±±ã¯ã€é™å²¡çœŒã¨å±±æ¢¨çœŒã«ã¾ãŸãŒã‚‹æ¨™é«˜3,776mã®å±±ã§ã™ã€‚ å¯Œå£«å±±ã¯ã€é™å²¡çœŒã¨å±±æ¢¨çœŒã«ã¾ãŸãŒã‚‹æ¨™é«˜3,776mã®å±±ã§ã™ã€‚ å¯Œå£«å±±ã¯ã€é™å²¡çœŒã¨å±±æ¢¨çœŒã«ã¾ãŸãŒã‚‹æ¨™é«˜
```

ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒ å‡ºåŠ›ï¼‰

```py
from huggingface_hub import InferenceClient

client = InferenceClient(model="http://localhost:8080")
response = client.text_generation(prompt="æ—¥æœ¬ã§ä¸€ç•ªé«˜ã„å±±ã¯ãªã‚“ã§ã™ã‹ï¼Ÿ", max_new_tokens=20, stream=True)
for token in response:
    print(token, end='')
```

å®Ÿè¡Œçµæœ:

```py
æ—¥æœ¬ã§ä¸€ç•ªé«˜ã„å±±ã¯ãªã‚“ã§ã™ã‹?
ãƒ™ã‚¹ãƒˆã‚¢ãƒ³ã‚µãƒ¼
å¯Œå£«å±±ã§ã™ã€‚
```

# çµ‚ã‚ã‚Šã«

ä»Šå›ã¯ã€æ‰‹å…ƒã§ LLM ã®æ¨è«–ç’°å¢ƒã‚’ç°¡å˜ã«æä¾›ã™ã‚‹æ–¹æ³•ã® 1 ã¤ã¨ã—ã¦ Hugging Face TGI ã‚’è©¦ã—ã¦ã¿ã¾ã—ãŸã€‚LLM ã«å¯¾ã™ã‚‹ä¸»è¦ãªæ“ä½œã‚„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ãŠã‚Šã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¾ã§æä¾›ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€éå¸¸ã«ä¾¿åˆ©ã«ä½¿ãˆãã†ã ãªã¨æ„Ÿã˜ã¾ã—ãŸã€‚ä¸€æ–¹ã§ã€ç´ æ•µã ãªï¼ã¨æ€ã£ãŸ OpenTelemetry ã‚’ç”¨ã„ãŸåˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã«ã¤ã„ã¦ã¯ç¢ºèªã—ãŸé™ã‚Šã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹å½“ãŸã‚‰ãšã©ã®ã‚ˆã†ã«è¨ˆè£…ã™ã‚‹ã®ã‹åˆ†ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã“ã®è¾ºã‚Šã¯ã€ã‚‚ã†å°‘ã—èª¿æŸ»ã‚’é€²ã‚ã¦ã¿ãŸã„ã¨æ€ã„ã¾ã™ã€‚

# å‚è€ƒ

https://huggingface.co/docs/text-generation-inference/index
